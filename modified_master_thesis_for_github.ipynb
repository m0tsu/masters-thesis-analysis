{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修士論文のコードを本当はアップロードしたかったのですが、修士論文が共同研究だった関係上守秘義務が発生し\n",
    "#守秘義務が2027年まで残っているので、やむなくChatGPTに\n",
    "#「修士論文で使用したコードを就活のポートフォリオとして掲載したいですが、修士論文のコードは守秘義務が課せられており使用できません。\n",
    "#そこで、以下のコードを守秘義務的に問題がないように、やりたいことは同じまま改変してください。」\n",
    "#と説明して変換してもらいました。全セルを同時に入力すると超絶省略した形でしか変換してくれなかったので各セルごとに変換してもらいましたが、\n",
    "#代わりに毎回importする超絶冗長コードになっている部分があります。\n",
    "#ですので、「なんかこういうことをやっていたんだなー」程度の雰囲気で読んでいただけると幸いです。\n",
    "\n",
    "#簡単に全体感を述べると、\n",
    "#1. データ形式の乱れの処理を行いました。\n",
    "#2. データを可視化してみてどう分析するかを考えました(これはこの後も適時行なっていました)。\n",
    "#3. 異常値の切り捨てを行いました。\n",
    "#4. 私が研究したデータは時系列データで、時系列的な欠損の補完や、正規分布への変換等を行いました。具体的には対数変換やYeo-Johnson変換を試しました。正規分布の確認はShapiro-WilkテストとKolmogorov-Smirnovテストを実施しています。\n",
    "#5. 特徴量間で高すぎる相関を示すものは取り除きました。\n",
    "#6. XGBoostで予測を行い、特徴量の重要度を求めました。\n",
    "#7. 特徴量の重要度に基づいて、AUCが最大になる特徴量の組み合わせを求めました。\n",
    "#他にもPyCaretを用いて様々な機械学習手法を同時に比較検討してみたり、LSTMで時系列方向での予測を行ったりしました。\n",
    "#そもそも時系列データなのになんでLSTM等の時系列手法ではなくXGBoostを使っているんだ？という疑問が湧くと思いますが、\n",
    "#1つ目の理由が、患者データの計測感覚が1日1回だったため前の状態が次の状態に与える影響の法則性が小さく時系列方向での周期性も見られにくかったため\n",
    "#2つ目の理由が、リアルワールドデータではありがちかもしれませんが、計測忘れによる空データに対応する必要があったためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# データ読み込みのためのパス設定\n",
    "data_file_path1 = 'path/to/your/datafile1.csv'\n",
    "data_file_path2 = 'path/to/your/datafile2.csv'\n",
    "\n",
    "# pandasを使用してCSVデータを読み込み\n",
    "data1 = pd.read_csv(data_file_path1, encoding='utf-8', sep=\",\", parse_dates=[1])  # データ1の読み込み\n",
    "data2 = pd.read_csv(data_file_path2, encoding='utf-8', sep=\",\", parse_dates=[-6])  # データ2の読み込み\n",
    "\n",
    "# データの深いコピーを作成\n",
    "data1_original = copy.deepcopy(data1)\n",
    "\n",
    "# 不要な列の削除と列名の変更\n",
    "data1 = data1.drop(['column1', 'column2', 'column3', 'column4', 'column5'], axis=1)\n",
    "data1 = data1.rename(columns={'old_name':'user'})\n",
    "\n",
    "# 新しい列の計算と追加\n",
    "data1['new_metric'] = data1['metric1'] / data1['metric2']\n",
    "data1['date'] = data1['datetime_column'].apply(lambda x: dt.date(x.year, x.month, x.day))\n",
    "data1['time'] = data1['datetime_column'].apply(lambda x: dt.time(x.hour, x.minute, x.second).strftime('%H:%M:%S'))\n",
    "data1['time'] = pd.to_datetime(data1['time']).dt.hour * 60 + pd.to_datetime(data1['time']).dt.minute\n",
    "data1['date'] = pd.to_datetime(data1['date'])\n",
    "data1['birth_date'] = pd.to_datetime(data1['birth_date'])\n",
    "data1['age'] = (data1['date'] - data1['birth_date']).astype('timedelta64[Y]')\n",
    "\n",
    "# ユーザー名の誤りを修正\n",
    "data1 = data1.replace({'user': {'incorrect_name1': 'correct_name1'}})\n",
    "data1 = data1.sort_values(['user', 'date']).reset_index(drop=True)\n",
    "\n",
    "# データの選択基準に基づくフィルタリング\n",
    "data1_filtered = copy.deepcopy(data1)\n",
    "data1_quality = copy.deepcopy(data1)\n",
    "criteria = (data1_filtered['quality_metric'] >= 90) & \\\n",
    "           (data1_filtered['metric3'] < 63) & \\\n",
    "           (data1_filtered['metric4'] < 100) & \\\n",
    "           ...  # 他の条件も同様に追加\n",
    "data1_filtered = data1_filtered[criteria]\n",
    "\n",
    "# 質の高いデータのみを選択\n",
    "data1_quality['quality_flag'] = 0\n",
    "data1_quality.loc[data1_filtered.index, 'quality_flag'] = 1\n",
    "\n",
    "# 不要な列の削除\n",
    "data1_filtered = data1_filtered.drop('metric5', axis=1)\n",
    "data1_quality = data1_quality.drop('metric5', axis=1)\n",
    "\n",
    "# 質の高いデータセットを特定\n",
    "data1_high_quality = data1[data1['quality_metric'] == 100].drop('metric5', axis=1)\n",
    "\n",
    "# pandasの設定変更\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ユーザーの各日付について最も品質が高いデータの中から平均的なデータのみを選択\n",
    "# 実行に時間がかかる可能性があります。\n",
    "# 一時的なデータセットにコピーしておきます。必要に応じて他のデータセットを使用できます。\n",
    "temp_dataset = copy.deepcopy(data_high_quality)\n",
    "user_list = temp_dataset['user'].unique()\n",
    "iteration_count = 0\n",
    "for user in user_list:\n",
    "    user_data = temp_dataset[temp_dataset['user'] == user]\n",
    "    date_list = user_data['date'].unique()\n",
    "    for date in date_list:\n",
    "        date_data = user_data[user_data['date'] == date]  # 各ユーザーの各日に測定したデータを選択\n",
    "        max_quality = date_data['quality_metric'].max()  # その日の最大品質値\n",
    "        max_quality_data = pd.DataFrame(date_data[date_data['quality_metric'] == max_quality])\n",
    "        if max_quality_data.shape[0] >= 1:\n",
    "            diff = (max_quality_data['metric3'] - max_quality_data['metric3'].mean()).abs()\n",
    "            max_quality_data = max_quality_data.loc[[diff.idxmin()]]  # 最も平均に近いデータを選択\n",
    "        if iteration_count == 0:\n",
    "            selected_data = max_quality_data\n",
    "        else:\n",
    "            selected_data = pd.concat([selected_data, max_quality_data])\n",
    "        iteration_count += 1\n",
    "\n",
    "selected_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# データの可視化：特定の数値データの分布をヒストグラムで表示\n",
    "sns.histplot(data=data_filtered, x='numeric_feature')\n",
    "plt.title('Distribution of Numeric Feature')\n",
    "plt.xlabel('Numeric Feature')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、ユーザーごとに日付の範囲を生成し、元のデータセットにマージして、\n",
    "#欠損日も含めた完全な時系列データセットを作成するためのものです。\n",
    "#具体的な変数名やデータセット名を一般化し、データの整形処理を行う方法を示します。\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# データの準備\n",
    "data_prepared = copy.deepcopy(data_filtered)\n",
    "\n",
    "# 各ユーザーの測定開始日と最終日を取得\n",
    "date_start = data_prepared.groupby('user')['date'].min()\n",
    "date_end = data_prepared.groupby('user')['date'].max()\n",
    "\n",
    "# 全ユーザーの日付範囲を生成して1つのデータフレームに集約\n",
    "date_list_all_users = pd.DataFrame(index=[], columns=['date', 'user'])\n",
    "for user_id in date_start.index:\n",
    "    date_range_each_user = pd.DataFrame(pd.date_range(start=date_start[user_id], end=date_end[user_id], freq='D'), columns=[\"date\"])\n",
    "    date_range_each_user['user'] = user_id\n",
    "    date_list_all_users = date_list_all_users.append(date_range_each_user, ignore_index=True)\n",
    "\n",
    "# 元のデータセットとマージして、欠損日も含む完全な時系列データセットを作成\n",
    "data_complete_timeseries = pd.merge(date_list_all_users, data_prepared, on=['date', 'user'], how='left')\n",
    "\n",
    "# 不要な列の削除\n",
    "data_complete_timeseries = data_complete_timeseries.drop(['datetime_column', 'birth_date_column'], axis=1)\n",
    "\n",
    "\n",
    "#この改変により、特定のプロジェクトやデータセットに依存する部分を抽象化し、\n",
    "#一般的なデータ処理のフローを示しています。data_filtered は前処理済みのデータセット、\n",
    "#data_complete_timeseries は欠損日を含む完全な時系列データセットを表しています。\n",
    "#具体的な列名（例えば datetime_column や birth_date_column）は、実際のデータセットの内容に合わせて適宜置き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、データセットに対して対数変換を適用し、特定の測定値の分布を正規化するプロセスを示しています。\n",
    "#さらに、各ユーザーごとに日付の範囲を生成し、元のデータセットにマージして完全な時系列データセットを作成します。\n",
    "#具体的な変数名やデータセット名を一般化し、データの変換と整形処理を行う方法を示します。\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# データの準備\n",
    "data_processed = copy.deepcopy(data_complete_timeseries)\n",
    "\n",
    "# 不要な列の削除\n",
    "data_processed = data_processed.drop('quality_metric', axis=1)\n",
    "\n",
    "# 対数変換を適用する列名のリスト\n",
    "metrics_to_transform = ['metric_a', 'metric_b', 'metric_c', 'metric_d', 'metric_e', 'metric_f', 'metric_g', 'metric_h', 'metric_i']\n",
    "\n",
    "# 対数変換の適用\n",
    "for metric in metrics_to_transform:\n",
    "    data_processed['Ln_' + metric] = np.log(data_processed[metric] + 1)  # 0を避けるために+1\n",
    "    data_processed = data_processed.drop(metric, axis=1)\n",
    "\n",
    "# 特定の条件に基づくデータのフィルタリング\n",
    "filtered_criteria = (data_processed['Ln_metric_a'] < 5.5) & \\\n",
    "                    (data_processed['Ln_metric_b'] < 5.5) & \\\n",
    "                    ...  # 他の条件も同様に追加\n",
    "data_processed = data_processed[filtered_criteria]\n",
    "\n",
    "# 各ユーザーの測定開始日と最終日を取得\n",
    "date_start = data_processed.groupby('user')['date'].min()\n",
    "date_end = data_processed.groupby('user')['date'].max()\n",
    "\n",
    "# 全ユーザーの日付範囲を生成して1つのデータフレームに集約\n",
    "date_list_all_users = pd.DataFrame(index=[], columns=['date', 'user'])\n",
    "for user_id in date_start.index:\n",
    "    date_range_each_user = pd.DataFrame(pd.date_range(start=date_start[user_id], end=date_end[user_id], freq='D'), columns=[\"date\"])\n",
    "    date_range_each_user['user'] = user_id\n",
    "    date_list_all_users = date_list_all_users.append(date_range_each_user, ignore_index=True)\n",
    "\n",
    "# 元のデータセットとマージして、欠損日も含む完全な時系列データセットを作成\n",
    "data_with_all_dates = pd.merge(date_list_all_users, data_processed, on=['date', 'user'], how='left')\n",
    "\n",
    "#このコードは、特定の測定値に対数変換を適用し、その後でデータセットを特定の条件に基づいてフィルタリングします。\n",
    "#また、各ユーザーについて可能なすべての日付を含むデータフレームを生成し、これを元のデータセットにマージしています。\n",
    "#これにより、欠損値をNaNとして含む完全な時系列データセットが作成されます。\n",
    "#実際に使用する際には、metrics_to_transform のリストやフィルタリング条件を実際のデータに合わせて調整してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、時系列データに対して過去数日間の情報を特徴量として組み込む処理と、\n",
    "#その後のデータの集約（平均や標準偏差の計算）を行うプロセスを示しています。\n",
    "#具体的な変数名やデータセット名を一般化し、データの変換と集約処理を行う方法を示します。\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# 特徴量リストの定義\n",
    "feature_list = pd.Series(['feature_a', 'Ln_feature_b', 'Ln_feature_c', 'Ln_feature_d', 'Ln_feature_e', 'Ln_feature_f', 'Ln_feature_g',\n",
    "       'Ln_feature_h', 'Ln_feature_i', 'Ln_feature_j', 'Ln_feature_k'])\n",
    "\n",
    "# ユーザーごとのデータカウントと特定の条件を満たすユーザーの選定\n",
    "user_days_count = data_with_all_dates.groupby(['user'])['Ln_feature_b'].count()\n",
    "users_with_enough_data = user_days_count[user_days_count >= 8].index.tolist()\n",
    "\n",
    "# 過去のデータを特徴量として組み込む処理\n",
    "multidays = 7\n",
    "for user_id in users_with_enough_data:\n",
    "  user_data = data_with_all_dates[data_with_all_dates['user'] == user_id]\n",
    "  for j in range(1, multidays):\n",
    "    shifted_data = user_data[feature_list].shift(j).add_suffix('_' + str(j) + 'day(s)_before')\n",
    "    user_data = pd.concat([user_data, shifted_data], axis=1)\n",
    "\n",
    "# 集約処理：各特徴量に対する平均と標準偏差の計算\n",
    "for feature in feature_list:\n",
    "  aggregated_features = user_data.filter(regex=feature).copy()\n",
    "  user_data[feature + '_average'] = aggregated_features.mean(axis=1)\n",
    "  user_data[feature + '_std'] = aggregated_features.std(axis=1)\n",
    "  user_data[feature + '_cv'] = user_data[feature + '_std'] / user_data[feature + '_average']\n",
    "\n",
    "# 特定の文字列を含む/含まない列名を取得し、データをフィルタリング\n",
    "character = 'before'\n",
    "filtered_columns = [col for col in user_data.columns if character not in col]\n",
    "data_aggregated = user_data[filtered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、あるデータセットに対して状態を数値コードに置き換え、\n",
    "#特定のカテゴリーに関連する列を除外し、さらにユーザーごとに日付順にソートする処理を行い、\n",
    "#最後に各ユーザーの平均値を基準にデータを正規化するプロセスを示しています。\n",
    "#具体的な変数名やデータセット名を一般化し、データの前処理と変換処理を行う方法を示します。\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# データの準備と前処理\n",
    "data_prepared = copy.deepcopy(data)\n",
    "\n",
    "# 状態を数値コードに置き換え\n",
    "data_prepared = data_prepared.replace({'status_column': {'status1': 0, 'status2': 1, 'status3': 2, 'status4': 3, 'status5': 4}})\n",
    "data_prepared = data_prepared.drop(['status2', 'status3', 'status4', 'status5', 'extra_column'], axis=1)\n",
    "\n",
    "# 特定の文字列を含む列の除去\n",
    "exclude_character = 'exclude_term'\n",
    "columns_to_include = [col for col in data_prepared.columns if exclude_character not in col]\n",
    "data_prepared = data_prepared[columns_to_include]\n",
    "\n",
    "# 選択された列でデータセットをフィルタリング\n",
    "selected_columns = ['user_id', 'date', 'status_column', 'metric1', 'metric2', 'metric3', 'metric4']\n",
    "data_selected = data_prepared[selected_columns]\n",
    "\n",
    "# ユーザーごとに日付順にソート\n",
    "data_selected_sorted = data_selected.sort_values(['user_id', 'date'])\n",
    "\n",
    "# 各ユーザーごとに平均値を基準にデータを正規化\n",
    "for user_id in data_selected_sorted['user_id'].unique():\n",
    "    user_data = data_selected_sorted[data_selected_sorted['user_id'] == user_id]\n",
    "    mean_values = user_data.mean(axis=0)\n",
    "    for col in ['metric1', 'metric2', 'metric3', 'metric4']:\n",
    "        user_data[col] = user_data[col] - mean_values[col]\n",
    "    if 'normalized_data' in locals():\n",
    "        normalized_data = pd.concat([normalized_data, user_data])\n",
    "    else:\n",
    "        normalized_data = user_data\n",
    "\n",
    "normalized_data = normalized_data.reset_index(drop=True)\n",
    "\n",
    "# 結果の表示や保存処理\n",
    "# display(normalized_data.head())\n",
    "# normalized_data.to_csv('path/to/your/normalized_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、データの読み込み、特定の列の削除、および特定の条件に基づいて列を選択する処理を行うスニペットです。\n",
    "#データセットの名前や操作を一般化し、データの読み込みから列の選択までのプロセスを示します。\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "# データファイルのパス（一般化）\n",
    "data_file_path = 'path/to/your/processed_data.csv'\n",
    "\n",
    "# pandasを使用してデータを読み込み\n",
    "processed_data = pd.read_csv(data_file_path, encoding='utf-8', sep=\",\", parse_dates=[1])\n",
    "\n",
    "# 特定の列を削除（一般的な列名に置き換え）\n",
    "processed_data = processed_data.drop(['unnecessary_column1', 'datetime_column', 'birth_date_column'], axis=1)\n",
    "\n",
    "# 時間を数値に変換\n",
    "processed_data['time_numeric'] = pd.to_datetime(processed_data['time_column']).dt.hour * 60 + pd.to_datetime(processed_data['time_column']).dt.minute\n",
    "\n",
    "# 特定の値の置換（一般化）\n",
    "processed_data = processed_data.replace({'user_column': {'incorrect_value1': 'correct_value1'}})\n",
    "\n",
    "# 特定の文字列を含む/含まない列の選択と除去\n",
    "exclude_terms = ['exclude_term1', 'exclude_term2', 'exclude_term3', 'exclude_term4', 'exclude_term5']\n",
    "for term in exclude_terms:\n",
    "    columns_to_exclude = [col for col in processed_data.columns if term in col]\n",
    "    processed_data = processed_data.drop(columns_to_exclude, axis=1)\n",
    "\n",
    "# 必要な列のみを選択（一般化）\n",
    "required_columns = ['date', 'user_column', 'time_numeric', 'age_column']\n",
    "processed_data = processed_data[required_columns + [col for col in processed_data.columns if 'Ln_' in col]]\n",
    "\n",
    "# データをユーザーと日付でソート\n",
    "processed_data = processed_data.sort_values(['user_column', 'date'])\n",
    "\n",
    "# 結果の表示や保存処理\n",
    "# display(processed_data.head())\n",
    "# processed_data.to_csv('path/to/your/final_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、特定のデータセットから選択された特徴量を組み合わせて新しいデータセットを作成し、\n",
    "#その後にデータの前処理を行うプロセスを示しています。\n",
    "#具体的な変数名やデータセット名を一般化し、データの結合、欠損値の処理、カテゴリカル変数の数値化を行う方法を示します。\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# データの準備と結合\n",
    "feature_data = copy.deepcopy(processed_data)  # processed_dataは前のステップで前処理されたデータセット\n",
    "target_data = target_dataset[['user_id', 'date', 'target_variable']]  # target_datasetは目的変数を含むデータセット\n",
    "\n",
    "# データセットの結合\n",
    "merged_dataset = pd.merge(left=target_data, right=feature_data, how='left', on=['user_id', 'date'])\n",
    "\n",
    "# ユーザーごとの年齢情報を結合\n",
    "user_age_info = feature_data.groupby('user_id')['age'].apply(lambda x: x.mode()).reset_index()\n",
    "user_age_info = user_age_info[user_age_info['level_1'] == 0][['user_id', 'age']].set_index('user_id')\n",
    "merged_dataset = pd.merge(merged_dataset, user_age_info, how='left', on='user_id')\n",
    "merged_dataset['age'] = merged_dataset['age'].fillna(merged_dataset['age_y']).drop('age_y', axis=1)\n",
    "\n",
    "# データの整理\n",
    "merged_dataset.drop('date', axis=1, inplace=True)\n",
    "\n",
    "# ユーザーIDの数値化\n",
    "label_encoder = LabelEncoder()\n",
    "merged_dataset['user_id_encoded'] = label_encoder.fit_transform(merged_dataset['user_id'])\n",
    "merged_dataset.drop('user_id', axis=1, inplace=True)\n",
    "\n",
    "# 欠損値の処理\n",
    "merged_dataset.dropna(subset=['target_variable'], inplace=True)\n",
    "\n",
    "# 数値特徴量のリスト作成\n",
    "numeric_features = [col for col in merged_dataset.columns if col != 'target_variable']\n",
    "\n",
    "# pandasの設定変更\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# 最終的なデータセットの準備\n",
    "final_dataset = copy.deepcopy(merged_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、データセットにYeo-Johnson変換を適用し、\n",
    "#その後に統計モデルを用いて変換後のデータセットの変数間の関係を分析するプロセスを示しています。\n",
    "#Yeo-Johnson変換は、データの正規性を改善するために使用されます。\n",
    "#具体的な変数名やデータセット名を一般化し、データの変換と分析処理を行う方法を示します。\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# データの準備\n",
    "features_data = copy.deepcopy(features_dataset)  # features_datasetは特徴量を含むデータセット\n",
    "target_data = copy.deepcopy(target_dataset)  # target_datasetは目的変数を含むデータセット\n",
    "\n",
    "# 無限大の値をNaNに置き換え\n",
    "features_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "target_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Yeo-Johnson変換の適用\n",
    "transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for column in features_data.columns[2:]:\n",
    "    transformed_data = scaler.fit_transform(features_data[[column]].dropna())\n",
    "    transformed_data = transformer.fit_transform(transformed_data)\n",
    "    features_data[column] = transformed_data\n",
    "\n",
    "for column in target_data.columns[:-3]:\n",
    "    transformed_data = scaler.fit_transform(target_data[[column]].dropna())\n",
    "    transformed_data = transformer.fit_transform(transformed_data)\n",
    "    target_data[column] = transformed_data\n",
    "\n",
    "# 統計モデルによる分析\n",
    "pvalues = pd.DataFrame(index=features_data.columns[2:], columns=target_data.columns[:-3])\n",
    "for target_col in target_data.columns[:-3]:\n",
    "    for feature_col in features_data.columns[2:]:\n",
    "        merged_data = pd.merge(target_data[['user_id', 'date', target_col]],\n",
    "                               features_data[['user_id', 'date', feature_col]],\n",
    "                               on=['user_id', 'date'], how='inner').dropna()\n",
    "        X = sm.add_constant(merged_data[feature_col])\n",
    "        Y = merged_data[target_col]\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        pvalues.loc[feature_col, target_col] = model.pvalues[1]\n",
    "\n",
    "# pvaluesの表示や分析\n",
    "# display(pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、変換後のデータに対して統計的分析を行い、\n",
    "#特定の変数間の関係性を評価し、残差の正規分布を検証するプロセスを示しています。\n",
    "#具体的な変数名やデータセット名を一般化し、データの分析と残差の検証を行う方法を示します。\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "\n",
    "# 分析対象の目的変数と説明変数を選択\n",
    "target_variable = 'target_metric'  # 例: 'target_metric' は分析したい目的変数\n",
    "explanatory_variable = 'transformed_feature'  # 例: 'transformed_feature' は分析したい説明変数\n",
    "\n",
    "# 分析用のデータセットの準備\n",
    "analysis_dataset = pd.merge(target_data_subset, transformed_feature_data, how='left', on=['user_id', 'date'])\n",
    "\n",
    "# 年齢情報の統合\n",
    "user_age_info = transformed_feature_data.groupby('user_id')['age'].apply(lambda x: x.mode()).reset_index()\n",
    "user_age_info = user_age_info[user_age_info['level_1'] == 0].set_index('user_id')\n",
    "analysis_dataset = pd.merge(analysis_dataset, user_age_info, on='user_id', how='left')\n",
    "analysis_dataset['age'] = analysis_dataset['age'].fillna(analysis_dataset['age_y']).drop(['age_y'], axis=1)\n",
    "\n",
    "# 不要な列の削除\n",
    "analysis_dataset = analysis_dataset.drop(['date', 'user_id'], axis=1).dropna()\n",
    "\n",
    "# 統計モデルのフィッティング\n",
    "X = sm.add_constant(analysis_dataset[explanatory_variable])\n",
    "Y = analysis_dataset[target_variable]\n",
    "model = sm.OLS(Y, X).fit()\n",
    "\n",
    "# 結果の表示\n",
    "print(model.summary())\n",
    "\n",
    "# 回帰プロットと残差の正規性検定\n",
    "sns.jointplot(data=analysis_dataset, x=explanatory_variable, y=target_variable, kind='reg')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(model.resid, bins=40)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "stats.probplot(model.resid, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.show()\n",
    "\n",
    "# 残差の正規性検定\n",
    "shapiro_test = stats.shapiro(model.resid)\n",
    "ks_test = stats.kstest(model.resid, \"norm\")\n",
    "print(\"Shapiro-Wilk Test:\", shapiro_test)\n",
    "print(\"Kolmogorov-Smirnov Test:\", ks_test)\n",
    "\n",
    "#この改変により、目的変数と説明変数の関係性を評価し、残差の正規性を検証するための統計的手法を適用しています。\n",
    "#Yeo-Johnson変換後の変数を用いて統計モデルを構築し、回帰プロット、残差分布、Q-Qプロットを用いて分析結果を視覚化し、\n",
    "#残差の正規性についてShapiro-WilkテストとKolmogorov-Smirnovテストを実施しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、XGBoostを用いて特定の目的変数に対する二値分類を行うためのデータ準備プロセスを示しています。\n",
    "#具体的な変数名やデータセット名を一般化し、データの準備からモデルのトレーニングまでのプロセスを示します。\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# 目的変数を設定\n",
    "target_variable = 'binary_outcome'  # 例えば、'binary_outcome'は分析したい二値の目的変数\n",
    "\n",
    "# 分析用データセットの準備\n",
    "merged_data = pd.merge(left=pro_target_data, right=transformed_features_data, how='left', on=['user_id', 'date'])\n",
    "\n",
    "# ユーザーごとの年齢情報を結合\n",
    "user_age_info = transformed_features_data.groupby('user_id')['age'].apply(lambda x: x.mode()).reset_index()\n",
    "user_age_info = user_age_info[user_age_info['level_1'] == 0].set_index('user_id')\n",
    "merged_data = pd.merge(merged_data, user_age_info, on='user_id', how='left')\n",
    "merged_data['age'] = merged_data['age'].fillna(merged_data['age_y']).drop(['age_y'], axis=1)\n",
    "\n",
    "# 不要な列の削除と欠損値の処理\n",
    "merged_data = merged_data.drop(['date', 'user_id'], axis=1).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# 目的変数の二値化\n",
    "merged_data[target_variable] = (merged_data[target_variable] > 0).astype(int)\n",
    "\n",
    "# データセットをトレーニングセットとテストセットに分割\n",
    "X = merged_data.drop(target_variable, axis=1)\n",
    "y = merged_data[target_variable]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoostモデルのトレーニング\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# トレーニング完了後、X_testを使用して予測を行い、性能を評価することが可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、データセット内の特定の目的変数の分布をヒストグラムで視覚化するためのスニペットです。\n",
    "#具体的な変数名を一般化し、データの可視化を行う方法を示します。\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 目的変数の分布をヒストグラムで表示\n",
    "sns.histplot(data=processed_dataset, x='binary_outcome')\n",
    "plt.title('Distribution of Binary Outcome')\n",
    "plt.xlabel('Binary Outcome')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、XGBoost分類器を用いた二値分類問題の解決と、結果の評価を行うプロセスを示しています。\n",
    "#具体的な変数名やデータセット名を一般化し、モデルのトレーニングと評価を行う方法を示します。\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 特徴量と目的変数を設定\n",
    "features = analysis_dataset.drop('binary_outcome', axis=1)\n",
    "outcome = analysis_dataset['binary_outcome']\n",
    "\n",
    "# データセットをトレーニングセットとテストセットに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, outcome, test_size=0.25, random_state=42)\n",
    "\n",
    "# XGBoost分類器のトレーニング\n",
    "xgb_classifier = XGBClassifier(tree_method='gpu_hist', max_depth=4, learning_rate=0.01)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# テストセットに対する予測確率の計算\n",
    "probabilities = xgb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC AUCスコアの計算\n",
    "auc_score = roc_auc_score(y_test, probabilities)\n",
    "\n",
    "# ROCカーブの描画\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, probabilities)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(false_positive_rate, true_positive_rate, label=f'AUC (XGBoost) = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、XGBoost分類器を用いた機械学習モデルのトレーニングと、\n",
    "#モデルの特徴量重要度の可視化を行うプロセスを示しています。\n",
    "#具体的な変数名やデータセット名を一般化し、モデルのトレーニングと特徴量の重要度の評価を行う方法を示します。\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 目的変数の設定\n",
    "binary_target = 'binary_target_variable'\n",
    "\n",
    "# 分析用データセットの準備\n",
    "analysis_dataset = prepared_dataset.dropna().reset_index(drop=True)\n",
    "X = analysis_dataset.drop(binary_target, axis=1)\n",
    "y = analysis_dataset[binary_target].astype(int)\n",
    "\n",
    "# データセットをトレーニングセットとバリデーションセットに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost分類器のトレーニング\n",
    "xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 特徴量重要度のプロット\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "xgb.plot_importance(xgb_classifier, ax=ax, importance_type='gain', show_values=True, title='Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#このコードは、複数の患者報告結果(PRO)指標に対して、\n",
    "#特定の生理的特徴量（ここではanbai特徴量として抽象化）を追加していく過程での\n",
    "#AUC（エリア・アンダー・ザ・カーブ）の平均値と標準誤差（SE）を計算し、\n",
    "#それらの結果を格納する複数のDataFrameを作成するプロセスを示しています。\n",
    "#具体的な変数名やデータセット名を一般化し、データ分析の結果を整理し、可視化する方法を示します。\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# 各指標（PRO）に対する生理的特徴量（anbai特徴量）の影響を評価\n",
    "pro_indices = ['PRO1', 'PRO2', 'PRO3']  # PRO指標のリスト\n",
    "pro_indices_renamed = ['PRO1 renamed', 'PRO2 renamed', 'PRO3 renamed']  # 変更後のPRO指標名のリスト\n",
    "features = ['feature1', 'feature2', 'feature3']  # 生理的特徴量のリスト\n",
    "\n",
    "# 結果を格納するDataFrameの準備\n",
    "result_1 = pd.DataFrame(index=pro_indices_renamed, columns=range(1, len(features) + 1)).fillna(0)\n",
    "result_2 = pd.DataFrame(index=pro_indices_renamed, columns=range(1, len(features) + 1)).fillna(0)\n",
    "result_3 = pd.DataFrame(index=pro_indices_renamed, columns=['mean', 'SE', 'Feature Count'] + [f'Feature {i+1}' for i in range(len(features))]).fillna('NA')\n",
    "result_4 = pd.DataFrame(index=pro_indices_renamed, columns=[f'Feature {i+1}' for i in range(len(features))]).fillna('NA')\n",
    "\n",
    "# 分析処理（例示）\n",
    "for pro_index in pro_indices:\n",
    "    # ここで各PRO指標についてのAUC計算や特徴量の重要度評価などの分析を行う\n",
    "    # 分析結果をresult_1, result_2, result_3, result_4に格納する\n",
    "    pass\n",
    "\n",
    "# 分析結果の可視化や保存処理\n",
    "# 例: 特徴量ごとのAUC平均値のプロット\n",
    "for pro_index_renamed in pro_indices_renamed:\n",
    "    plt.plot(result_1.loc[pro_index_renamed], label=pro_index_renamed)\n",
    "\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('AUC Mean')\n",
    "plt.title('AUC Mean by Number of Features for Each PRO')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(自分のコメント)この後、特徴量を重要度順に並べて(例: A, B, C, D...)\n",
    "#インプットデータをA→A+B→A+B+C→...と増やしていき、あるラインを超えると不要な特徴量が多すぎて予測精度が下がるので\n",
    "#そこでデータの追加をストップして最もAUCが高い特徴量の組み合わせを求めていました。\n",
    "#理論上は特徴量の組み合わせは2^(特徴量の種類)だけあるので全部試せたら良いのですが、\n",
    "#マシンのスペック的に難しかったので重要度順に並べることで簡易的に最適な特徴量の組み合わせを実装していました。\n",
    "#ChatGPTに変換をお願いしたところ、コードが長すぎて下のようにかなり省略しすぎた形になってしまったので、何をしたのかだけここまでの説明で掴んでいただけると幸いです。\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import RepeatedKFold, train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PRO指標のリスト\n",
    "pro_list = ['PRO1', 'PRO2', 'PRO3']\n",
    "\n",
    "# 分析と結果保存の設定\n",
    "n_splits = 5\n",
    "n_repeats = 100\n",
    "folder_id = 'your_folder_id_here'  # Google Drive内の保存先フォルダID\n",
    "\n",
    "# 特徴量の重要度とAUC評価の分析\n",
    "for pro in pro_list:\n",
    "    # 分析用データセットの準備\n",
    "    target = pro\n",
    "    dataset = prepare_dataset(pro)  # データセット準備関数は仮想的なものです\n",
    "    X, y = dataset.drop(target, axis=1), dataset[target]\n",
    "\n",
    "    # 特徴量選択とモデル評価を繰り返し実行\n",
    "    auc_results = evaluate_features(X, y, n_splits, n_repeats)  # 特徴量評価関数は仮想的なものです\n",
    "\n",
    "    # Google Driveに結果を保存\n",
    "    save_to_drive(auc_results, pro, folder_id)  # 結果保存関数は仮想的なものです\n",
    "\n",
    "def prepare_dataset(pro):\n",
    "    # データセットの準備に関するコード\n",
    "    return dataset\n",
    "\n",
    "def evaluate_features(X, y, n_splits, n_repeats):\n",
    "    # 特徴量の重要度とAUCの評価に関するコード\n",
    "    return auc_results\n",
    "\n",
    "def save_to_drive(data, pro, folder_id):\n",
    "    # Google Driveにデータを保存するコード\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
